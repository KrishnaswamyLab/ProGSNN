{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be662395-4599-4e23-9d99-0b72aaa3f714",
   "metadata": {},
   "source": [
    "# MDFP dataset generation\n",
    "\n",
    "## TODO\n",
    "- if scaling data, pickle the mins and maxs too, for easy inverse transformation later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5e588e-12fd-41fc-890d-5d4ae8ff8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset_subclasses import DatasetMDFP\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import mdtraj as md\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18d608-32a3-4d18-8d7d-5d9a8dfa39e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Function: DE Shaw data folder parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2942c1a2-9abd-4a66-819c-5a8b54a7b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deshaw_data_info(deshaw_folderpath):\n",
    "    \"\"\"\n",
    "    DE Shaw pdb files are in a weird folder structure\n",
    "    and file naming convention. This function walks\n",
    "    through a DE Shaw data folder and generates a\n",
    "    dictionary of lists holding useful file info, all\n",
    "    in the order of the sorted MD simulation timesteps.\n",
    "    \"\"\"\n",
    "    # deshaw pdb files are grouped in subfolders\n",
    "    deshaw_subfolders = sorted([\n",
    "        f.path for f in os.scandir(deshaw_folderpath) \\\n",
    "        if f.is_dir()\n",
    "    ])\n",
    "    n_subf = len(deshaw_subfolders)\n",
    "    records = {\n",
    "        'pdb_filepaths': [],\n",
    "        'suffix_vals': [],\n",
    "        'timestamps': []\n",
    "    }\n",
    "    subf_records_l = [None] * n_subf\n",
    "    \n",
    "    # extract info from each pdb file, by subfolder\n",
    "    for j, deshaw_subf in enumerate(deshaw_subfolders):\n",
    "        subf_files = os.listdir(deshaw_subf)\n",
    "        n = len(subf_files)\n",
    "        subf_records = {\n",
    "            'pdb_filepaths': [None] * n,\n",
    "            'suffix_vals': [None] * n,\n",
    "            'timestamps': [None] * n\n",
    "        }\n",
    "        \n",
    "        for i, pdb_filename in enumerate(subf_files):\n",
    "            a, b = pdb_filename.split('_')\n",
    "            a, val = b.split('-')\n",
    "            suffix_val = val.split('.')[0]\n",
    "            # int(suffix_val) is 0-2 microseconds\n",
    "            t = (int(a) * 1e4 + int(suffix_val)) / 1e4\n",
    "            subf_records['pdb_filepaths'][i] = f'{deshaw_subf}/{pdb_filename}'\n",
    "            subf_records['suffix_vals'][i] = suffix_val\n",
    "            subf_records['timestamps'][i] = t\n",
    "        \n",
    "        # sort subfolder info lists in timestamp order\n",
    "        for k, v in subf_records.items():\n",
    "            subf_records[k] = [\n",
    "                x for (_, x) \\\n",
    "                in sorted(zip(\n",
    "                    subf_records['timestamps'], \n",
    "                    subf_records[k]\n",
    "                ))\n",
    "            ]\n",
    "        subf_records_l[j] = subf_records\n",
    "    \n",
    "    # create master records dict (all in sorted timestamp order)\n",
    "    for k in records.keys():\n",
    "        for sr in subf_records_l:\n",
    "            records[k].extend(sr[k])\n",
    "    return records\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269a975-50d3-49bf-8e65-a94376a3b0b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Class: MDFP data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af4ad41-6a70-4d83-ab5b-2b392172c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessorMDFP:\n",
    "    \"\"\"\n",
    "    Processes an ATLAS or DE Shaw dataset, passed \n",
    "    as an MDTraj `Trajectory`, into a `DatasetMDFP` (a\n",
    "    subclass of `torch.utils.data.Dataset`).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 traj,\n",
    "                 dtype=torch.float32,\n",
    "                 minmaxscale=False):\n",
    "        self.traj = traj\n",
    "        self.dtype = dtype\n",
    "        self.minmaxscale = minmaxscale\n",
    "\n",
    "    \n",
    "    def _get_mdfp_feat_matrix(self):\n",
    "        \"\"\"\n",
    "        Calculates a 'Molecular Dynamics FingerPrint' (MDFP) feature\n",
    "        matrix for a protein trajectory, using the analysis module of the\n",
    "        MDTraj package.\n",
    "    \n",
    "        To reduce the number of features, note that 'contact_dists' is \n",
    "        usually a large number (all pairwise distances between residues).\n",
    "    \n",
    "        Note if minmaxscale=True, the scaler is trained on the entire dataset,\n",
    "        not just a train subset.\n",
    "        \n",
    "        This is based on the MDFP concept introduced in Riniker 2017.\n",
    "        https://doi.org/10.1021/acs.jcim.6b00778\n",
    "        https://github.com/rinikerlab/mdfptools/blob/master/examples/Example.ipynb\n",
    "        \"\"\"\n",
    "        \n",
    "        # track computation time\n",
    "        time_0 = time.time()\n",
    "        \n",
    "        # distances\n",
    "        ctr_mass = md.compute_center_of_mass(self.traj) # shape = (n_frames, 3)\n",
    "        contact_dists, res_idx = md.compute_contacts( \n",
    "            self.traj, \n",
    "            contacts='all', \n",
    "            scheme='closest-heavy'\n",
    "        ) # contact_dists shape = (n_frames, n_contacts)\n",
    "        \n",
    "        # radius of gyration, SASA\n",
    "        rg = md.compute_rg(self.traj) # shape = (n_frames,)\n",
    "        sasa = md.shrake_rupley(self.traj).sum(axis=1) # shape = (n_frames,)\n",
    "        \n",
    "        # torsion angles\n",
    "        phi_idx, phi_angles = md.compute_phi(self.traj) # shape = (n_frames, n_angles)\n",
    "        psi_idx, psi_angles = md.compute_psi(self.traj) # shape = (n_frames, n_angles)\n",
    "        omega_idx, omega_angles = md.compute_omega(self.traj) # shape = (n_frames, n_angles)\n",
    "        \n",
    "        # K-S hydrogen bond energy (HBE)\n",
    "        # `kabsch_sander` returns list (of len = n_frames) of matrices of shape = (n_residues, n_residues)\n",
    "        ks_hbe = md.kabsch_sander(self.traj) \n",
    "        # reduce this feature into mean, median, st. dev. HBE within each frame,\n",
    "        # for NONZERO HBE values only\n",
    "        # this approach (for other features) is done in (Riniker 2017)\n",
    "        hbe_stats = { # each list has len = n_frames\n",
    "            'mean': [None] * len(ks_hbe),\n",
    "            'median': [None] * len(ks_hbe),\n",
    "            'stdev': [None] * len(ks_hbe)\n",
    "        }\n",
    "        for i, sm in enumerate(ks_hbe):\n",
    "            tril_mask = np.tril_indices_from(sm, k=-1)\n",
    "            v = np.array(sm[tril_mask]).squeeze()\n",
    "            v_nonzero = v[v != 0.0]\n",
    "            hbe_stats['mean'][i] = np.mean(v_nonzero)\n",
    "            hbe_stats['median'][i] = np.median(v_nonzero)\n",
    "            hbe_stats['stdev'][i] = np.std(v_nonzero)\n",
    "        \n",
    "        # NOT INCLUDED: thermodynamic properties\n",
    "        # dip_mom = md.dipole_moments(self.traj) # requires charges\n",
    "        # stat_die = md.static_dielectric(self.traj) # requires charges\n",
    "        # therm_exp = md.thermal_expansion_alpha_P(self.traj) # requires temp. and pot. energies\n",
    "        \n",
    "        # collect feature arrays in a tuple, and give all at least 2 dim\n",
    "        feat_tup = (\n",
    "            ctr_mass,\n",
    "            contact_dists,\n",
    "            np.expand_dims(rg, axis=1), \n",
    "            np.expand_dims(sasa, axis=1),\n",
    "            phi_angles,\n",
    "            psi_angles,\n",
    "            omega_angles,\n",
    "            np.expand_dims(np.array(hbe_stats['mean']), axis=1),\n",
    "            np.expand_dims(np.array(hbe_stats['median']), axis=1),\n",
    "            np.expand_dims(np.array(hbe_stats['stdev']), axis=1),\n",
    "        )\n",
    "        # check shapes if needed\n",
    "        # for f in feat_tup:\n",
    "        #     print(f.shape)\n",
    "        \n",
    "        # stack features into matrix of shape = (n_frames, n_features)\n",
    "        X = np.concatenate(feat_tup, axis=1)\n",
    "        print('final mdfp matrix shape (n_frames, n_features):', \n",
    "              X.shape)\n",
    "    \n",
    "        if self.minmaxscale:\n",
    "            mm_scaler = MinMaxScaler()\n",
    "            X = mm_scaler.fit_transform(X)\n",
    "            \n",
    "        # print computation time\n",
    "        time_elapsed = time.time() - time_0\n",
    "        print(f'feature matrix generated in: {time_elapsed // 60:.0f}min, {time_elapsed % 60:.1f}sec')\n",
    "    \n",
    "        return X\n",
    "\n",
    "    \n",
    "    def _get_mdfp_targets(self):\n",
    "        \"\"\"\n",
    "        Generates the targets (id, timestep,\n",
    "        and stacked xyz-coordinates vector)\n",
    "        from the `self.traj`.\n",
    "        \"\"\"\n",
    "        # frame ids\n",
    "        ids = torch.tensor(\n",
    "            np.arange(self.traj.n_frames), \n",
    "            dtype=torch.long\n",
    "        )\n",
    "    \n",
    "        # scaled timestep targets\n",
    "        timesteps = ids.clone().detach().type(self.dtype)\n",
    "        if self.minmaxscale:\n",
    "            timesteps = timesteps / self.traj.n_frames\n",
    "    \n",
    "        # residues' center xyz coords\n",
    "        residue_ctr_coords_l = [None] * len(self.traj)\n",
    "        for i, frame in enumerate(self.traj):\n",
    "            residue_ctr_coords = [None] * frame.n_residues\n",
    "            for j, residue in enumerate(frame.top.residues):\n",
    "                atom_indices = [atom.index for atom in residue.atoms]\n",
    "                # note that frame.xyz[0].shape = (n_atoms, 3)\n",
    "                atom_coords = frame.xyz[0][atom_indices] \n",
    "                mean_coords = np.mean(atom_coords, axis=0)\n",
    "                residue_ctr_coords[j] = mean_coords\n",
    "            ctr_coords_arr = np.row_stack(residue_ctr_coords)\n",
    "            residue_ctr_coords_l[i] = ctr_coords_arr\n",
    "    \n",
    "        if self.minmaxscale:\n",
    "            # stack all frames' coords row-wise and use scaler\n",
    "            all_coords = np.concatenate(residue_ctr_coords_l, axis=0)\n",
    "            mm_scaler = MinMaxScaler()\n",
    "            all_coords = mm_scaler.fit_transform(all_coords)\n",
    "            # split all-frames scaled array into a list of indiv. frame arrays\n",
    "            residue_ctr_coords_l = [\n",
    "                arr for arr in np.split(all_coords, self.traj.n_frames, axis=0)\n",
    "            ]\n",
    "    \n",
    "        # unroll (n_residue, 3)-arrays into vectors and convert to list of tensors\n",
    "        coords_tensors_l = [\n",
    "            torch.tensor(arr, dtype=self.dtype).T.reshape(-1) \\\n",
    "            for arr in residue_ctr_coords_l\n",
    "        ]\n",
    "    \n",
    "        targets_dict = {\n",
    "            'id': ids,\n",
    "            'timestep': timesteps,\n",
    "            'coords': coords_tensors_l,\n",
    "        }\n",
    "        return targets_dict\n",
    "\n",
    "\n",
    "    def get_mdfp_dataset(self):\n",
    "        \"\"\"\n",
    "        Calls internal functions above to generate\n",
    "        a dataset (inputs and targets dict) from \n",
    "        the `self.traj`.\n",
    "        \"\"\"\n",
    "        # inputs\n",
    "        X = self._get_mdfp_feat_matrix()\n",
    "        inputs = [\n",
    "            torch.tensor(x, dtype=self.dtype) \\\n",
    "            for x in X.tolist()\n",
    "        ]\n",
    "        \n",
    "        # targets\n",
    "        targets_dict = self._get_mdfp_targets()\n",
    "        \n",
    "        dataset_mdfp = DatasetMDFP(\n",
    "            inputs=inputs,\n",
    "            targets=targets_dict\n",
    "        )\n",
    "        return dataset_mdfp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d5780-54c3-40b8-8619-de78c8afe384",
   "metadata": {},
   "source": [
    "## Run\n",
    "- Note this example only loads a partial trajectory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "16afdf7b-7d1f-433b-bb44-3d9efe9649c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_DATA_FOLDER = \"YOUR_PATH_HERE\"\n",
    "DESHAW_DATA_FOLDER = \"YOUR_PATH_HERE\"\n",
    "DATASET_SAVE_FOLDER = \"YOUR_PATH_HERE\"\n",
    "PROTEIN_INFO = {\n",
    "    \"1bx7_A\": {\"dir\": \"1bx7_A_analysis\", \"kind\": \"atlas\"},\n",
    "    \"1bxy_A\": {\"dir\": \"1bxy_A_analysis\", \"kind\": \"atlas\"},\n",
    "    \"1ptq_A\": {\"dir\": \"1ptq_A_analysise\", \"kind\": \"atlas\"},\n",
    "    \"GB3\": {\"dir\": \"GB3\", \"kind\": \"deshaw\"},\n",
    "    \"BPTI\": {\"dir\": \"BPTI\", \"kind\": \"deshaw\"},\n",
    "    \"Ubiquitin\": {\"dir\": \"Ubiquitin\", \"kind\": \"deshaw\"}\n",
    "}\n",
    "MD_RUN_INDEX = 1\n",
    "TORCH_DTYPE = torch.float32\n",
    "MINMAXSCALE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33eca8-9f57-4ee0-bf87-c211972925dc",
   "metadata": {},
   "source": [
    "### Load trajectory\n",
    "\n",
    "Load one of:\n",
    "\n",
    "- Atlas, from two files, an `xtc` and a `pdb`.\n",
    "- DE Shaw, from a large set of `pdb` files stored in subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131b5c9d-c8c5-46c2-9270-4485be755fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 1bx7_A...\n",
      "1bx7_A trajectory created in: 0min, 0.1sec\n",
      "final mdfp matrix shape (n_frames, n_features): (1001, 1548)\n",
      "feature matrix generated in: 0min, 23.8sec\n",
      "1bx7_A dataset created in: 0min, 25.5sec\n",
      "Was data min-max scaled? False\n",
      "\n",
      "Processing 1bxy_A...\n",
      "1bxy_A trajectory created in: 0min, 0.0sec\n",
      "final mdfp matrix shape (n_frames, n_features): (1001, 1838)\n",
      "feature matrix generated in: 0min, 33.6sec\n",
      "1bxy_A dataset created in: 0min, 36.0sec\n",
      "Was data min-max scaled? False\n",
      "\n",
      "Processing 1ptq_A...\n",
      "1ptq_A trajectory created in: 0min, 0.0sec\n",
      "final mdfp matrix shape (n_frames, n_features): (1001, 1283)\n",
      "feature matrix generated in: 0min, 25.8sec\n",
      "1ptq_A dataset created in: 0min, 27.6sec\n",
      "Was data min-max scaled? False\n",
      "\n",
      "Processing GB3...\n",
      "GB3 trajectory created in: 1min, 55.9sec\n",
      "final mdfp matrix shape (n_frames, n_features): (9985, 1604)\n",
      "feature matrix generated in: 4min, 32.7sec\n",
      "GB3 dataset created in: 4min, 52.5sec\n",
      "Was data min-max scaled? False\n",
      "\n",
      "Processing BPTI...\n",
      "BPTI trajectory created in: 2min, 1.6sec\n",
      "final mdfp matrix shape (n_frames, n_features): (9985, 1719)\n",
      "feature matrix generated in: 4min, 38.4sec\n",
      "BPTI dataset created in: 4min, 59.5sec\n",
      "Was data min-max scaled? False\n",
      "\n",
      "Processing Ubiquitin...\n",
      "Ubiquitin trajectory created in: 21min, 34.6sec\n",
      "final mdfp matrix shape (n_frames, n_features): (9985, 2934)\n",
      "feature matrix generated in: 108min, 50.3sec\n",
      "Ubiquitin dataset created in: 125min, 36.2sec\n",
      "Was data min-max scaled? False\n"
     ]
    }
   ],
   "source": [
    "for name, info_d in PROTEIN_INFO.items():\n",
    "    # 1 \n",
    "    # create trajectory from data (and track time)\n",
    "    time_0 = time.time()\n",
    "    print(f'Processing {name}...')\n",
    "    \n",
    "    if info_d['kind'] == 'atlas':\n",
    "        traj = md.load(f\"{ATLAS_DATA_FOLDER}/{info_d['dir']}/{name}_R{MD_RUN_INDEX}.xtc\", \n",
    "                           top=f\"{ATLAS_DATA_FOLDER}/{info_d['dir']}/{name}.pdb\")\n",
    "    elif info_d['kind'] == 'deshaw':\n",
    "        # DE Shaw trajectory\n",
    "        # first parse DE Shaw data folder for data info\n",
    "        deshaw_records = get_deshaw_data_info(f\"{DESHAW_DATA_FOLDER}/{info_d['dir']}\")\n",
    "        # print(deshaw_records['pdb_filepaths'])\n",
    "        traj = md.load(deshaw_records['pdb_filepaths'])\n",
    "        \n",
    "    te = time.time() - time_0\n",
    "    print(f'\\t{name} trajectory created in: {te // 60:.0f}min, {te % 60:.1f}sec')\n",
    "\n",
    "    # 2 \n",
    "    # generate dataset\n",
    "    time_1 = time.time()\n",
    "    data_proc_mdfp = DataProcessorMDFP(\n",
    "        traj,\n",
    "        dtype=TORCH_DTYPE,\n",
    "        minmaxscale=MINMAXSCALE\n",
    "    )\n",
    "    dataset_mdfp = data_proc_mdfp.get_mdfp_dataset()\n",
    "    te2 = time.time() - time_1\n",
    "    print(f'\\t{name} dataset created in: {te2 // 60:.0f}min, {te2 % 60:.1f}sec')\n",
    "    print(f'\\tWas data min-max scaled? {MINMAXSCALE}')\n",
    "\n",
    "    # 3\n",
    "    # pickle the reference trajectory (MDTraj.Trajectory object)\n",
    "    reftraj_filename = f'{DATASET_SAVE_FOLDER}/reftraj_{name}.p'\n",
    "    os.makedirs(os.path.dirname(reftraj_filename), exist_ok=True)\n",
    "    with open(reftraj_filename, 'wb') as f:\n",
    "        pickle.dump(traj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # 4\n",
    "    # pickle the dataset\n",
    "    if MINMAXSCALE:\n",
    "        filename = f'{DATASET_SAVE_FOLDER}/scaled/mdfp_dataset_{name}_minmaxscaled.p'\n",
    "    else:\n",
    "        filename = f'{DATASET_SAVE_FOLDER}/unscaled/mdfp_dataset_{name}.p'\n",
    "        \n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(dataset_mdfp, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f6b47a-4905-47e8-9652-71ee1fa2d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([ 2.55e-06, -3.73e-06, -2.98e-06,  ..., -2.25e+00, -2.24e+00,\n",
      "         4.73e-01]), 'target': {'id': tensor(9984), 'timestep': tensor(9984.), 'coords': tensor([ 1.00e-01,  6.86e-01,  3.76e-01,  8.75e-01,  5.65e-01,  1.10e+00,\n",
      "         9.12e-01,  8.92e-01,  1.29e+00,  1.41e+00,  1.24e+00,  1.24e+00,\n",
      "         8.41e-01,  9.22e-01,  4.36e-01,  3.27e-01, -1.95e-02, -4.63e-01,\n",
      "        -3.54e-01, -8.16e-01, -7.16e-01, -9.59e-01, -5.04e-01, -8.72e-01,\n",
      "        -6.40e-01, -1.71e-01, -4.01e-01, -3.75e-01, -9.89e-02,  1.62e-01,\n",
      "        -1.82e-01,  2.05e-01,  6.44e-01,  5.48e-01,  2.64e-01,  8.71e-02,\n",
      "        -3.34e-01, -5.41e-01, -7.56e-01, -2.14e-01, -1.57e-01, -4.08e-01,\n",
      "        -5.30e-02,  1.27e-01,  1.34e-01,  1.87e-01,  9.85e-02, -3.46e-01,\n",
      "        -4.47e-01, -3.28e-01, -9.19e-01, -7.80e-01, -1.10e+00, -9.80e-01,\n",
      "        -8.71e-01, -2.97e-01, -6.76e-01, -9.17e-01, -5.86e-01, -4.36e-01,\n",
      "        -8.61e-02,  2.44e-01,  4.48e-01,  8.13e-01,  4.84e-01,  7.78e-01,\n",
      "         2.87e-01,  6.35e-01,  4.00e-01,  2.38e-01,  2.25e-01, -5.41e-01,\n",
      "        -8.55e-02, -4.75e-01, -6.71e-01, -6.26e-01, -1.07e+00, -1.06e+00,\n",
      "        -5.89e-01, -4.96e-01, -9.83e-02,  3.46e-01,  3.27e-01,  8.62e-01,\n",
      "         6.28e-01,  4.90e-01,  7.20e-02, -1.44e-01, -3.27e-01, -7.82e-01,\n",
      "        -7.51e-01, -1.28e+00, -9.45e-01, -1.26e+00, -1.03e+00, -1.04e+00,\n",
      "        -9.21e-01, -5.02e-01, -1.54e-01, -2.15e-01, -6.73e-01, -4.34e-01,\n",
      "         3.60e-02, -5.05e-01, -8.30e-01, -1.90e-01, -2.60e-01, -6.66e-01,\n",
      "        -6.03e-01, -4.50e-02, -3.77e-02,  2.88e-01,  3.00e-01,  8.75e-02,\n",
      "         5.19e-01,  7.36e-01,  3.27e-01,  8.18e-01,  2.81e-01,  7.67e-01,\n",
      "         3.10e-01,  6.48e-01,  1.03e+00,  8.66e-01,  8.51e-01,  3.46e-01,\n",
      "         5.08e-01,  1.74e-01, -1.36e-02,  5.74e-02, -4.93e-01, -4.67e-01,\n",
      "        -7.00e-01, -2.84e-01,  9.01e-02, -3.59e-01, -2.51e-01, -5.05e-01,\n",
      "        -1.07e+00, -7.89e-01, -3.44e-01, -1.69e-03,  4.10e-02,  5.05e-01,\n",
      "         3.75e-01,  8.56e-01,  9.64e-01,  1.03e+00,  1.55e+00,  1.73e+00,\n",
      "         2.02e+00,  2.18e+00, -6.49e-01, -4.59e-01, -3.99e-01, -3.44e-01,\n",
      "         5.81e-02, -8.63e-02,  4.56e-01,  5.73e-01,  6.69e-01,  2.61e-01,\n",
      "         6.26e-01,  8.14e-02,  3.58e-01,  9.42e-02,  1.96e-01, -9.34e-02,\n",
      "        -1.71e-01, -3.75e-01, -7.51e-01, -6.48e-01, -2.35e-01, -1.36e-01,\n",
      "        -2.09e-01,  2.58e-01,  2.31e-01,  8.37e-02,  4.10e-01,  6.97e-01,\n",
      "         4.39e-01,  5.35e-01,  1.10e+00,  1.09e+00,  8.31e-01,  9.54e-01,\n",
      "         1.34e+00,  1.07e+00,  1.26e+00,  8.49e-01,  9.63e-01,  1.08e+00,\n",
      "         6.81e-01,  2.81e-01,  2.02e-02, -3.06e-01, -8.20e-01, -1.04e+00,\n",
      "        -9.05e-01, -1.00e+00, -2.90e-01, -4.44e-01, -4.27e-01,  1.01e-02,\n",
      "        -1.73e-01, -7.51e-01, -6.22e-01, -6.46e-01, -1.03e+00, -1.03e+00,\n",
      "        -9.01e-01, -1.44e+00, -9.74e-01, -1.30e+00, -1.07e+00, -8.50e-01,\n",
      "        -8.21e-01, -5.62e-01, -3.60e-01, -2.44e-01,  3.85e-01,  2.31e-01,\n",
      "         7.76e-01,  6.82e-01,  6.00e-01,  1.18e+00,  7.83e-01,  1.10e+00])}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': tensor([ 2.55e-06, -3.73e-06, -2.98e-06,  ..., -2.25e+00, -2.24e+00,\n",
       "          4.73e-01]),\n",
       " 'target': {'id': tensor(9984),\n",
       "  'timestep': tensor(9984.),\n",
       "  'coords': tensor([ 1.00e-01,  6.86e-01,  3.76e-01,  8.75e-01,  5.65e-01,  1.10e+00,\n",
       "           9.12e-01,  8.92e-01,  1.29e+00,  1.41e+00,  1.24e+00,  1.24e+00,\n",
       "           8.41e-01,  9.22e-01,  4.36e-01,  3.27e-01, -1.95e-02, -4.63e-01,\n",
       "          -3.54e-01, -8.16e-01, -7.16e-01, -9.59e-01, -5.04e-01, -8.72e-01,\n",
       "          -6.40e-01, -1.71e-01, -4.01e-01, -3.75e-01, -9.89e-02,  1.62e-01,\n",
       "          -1.82e-01,  2.05e-01,  6.44e-01,  5.48e-01,  2.64e-01,  8.71e-02,\n",
       "          -3.34e-01, -5.41e-01, -7.56e-01, -2.14e-01, -1.57e-01, -4.08e-01,\n",
       "          -5.30e-02,  1.27e-01,  1.34e-01,  1.87e-01,  9.85e-02, -3.46e-01,\n",
       "          -4.47e-01, -3.28e-01, -9.19e-01, -7.80e-01, -1.10e+00, -9.80e-01,\n",
       "          -8.71e-01, -2.97e-01, -6.76e-01, -9.17e-01, -5.86e-01, -4.36e-01,\n",
       "          -8.61e-02,  2.44e-01,  4.48e-01,  8.13e-01,  4.84e-01,  7.78e-01,\n",
       "           2.87e-01,  6.35e-01,  4.00e-01,  2.38e-01,  2.25e-01, -5.41e-01,\n",
       "          -8.55e-02, -4.75e-01, -6.71e-01, -6.26e-01, -1.07e+00, -1.06e+00,\n",
       "          -5.89e-01, -4.96e-01, -9.83e-02,  3.46e-01,  3.27e-01,  8.62e-01,\n",
       "           6.28e-01,  4.90e-01,  7.20e-02, -1.44e-01, -3.27e-01, -7.82e-01,\n",
       "          -7.51e-01, -1.28e+00, -9.45e-01, -1.26e+00, -1.03e+00, -1.04e+00,\n",
       "          -9.21e-01, -5.02e-01, -1.54e-01, -2.15e-01, -6.73e-01, -4.34e-01,\n",
       "           3.60e-02, -5.05e-01, -8.30e-01, -1.90e-01, -2.60e-01, -6.66e-01,\n",
       "          -6.03e-01, -4.50e-02, -3.77e-02,  2.88e-01,  3.00e-01,  8.75e-02,\n",
       "           5.19e-01,  7.36e-01,  3.27e-01,  8.18e-01,  2.81e-01,  7.67e-01,\n",
       "           3.10e-01,  6.48e-01,  1.03e+00,  8.66e-01,  8.51e-01,  3.46e-01,\n",
       "           5.08e-01,  1.74e-01, -1.36e-02,  5.74e-02, -4.93e-01, -4.67e-01,\n",
       "          -7.00e-01, -2.84e-01,  9.01e-02, -3.59e-01, -2.51e-01, -5.05e-01,\n",
       "          -1.07e+00, -7.89e-01, -3.44e-01, -1.69e-03,  4.10e-02,  5.05e-01,\n",
       "           3.75e-01,  8.56e-01,  9.64e-01,  1.03e+00,  1.55e+00,  1.73e+00,\n",
       "           2.02e+00,  2.18e+00, -6.49e-01, -4.59e-01, -3.99e-01, -3.44e-01,\n",
       "           5.81e-02, -8.63e-02,  4.56e-01,  5.73e-01,  6.69e-01,  2.61e-01,\n",
       "           6.26e-01,  8.14e-02,  3.58e-01,  9.42e-02,  1.96e-01, -9.34e-02,\n",
       "          -1.71e-01, -3.75e-01, -7.51e-01, -6.48e-01, -2.35e-01, -1.36e-01,\n",
       "          -2.09e-01,  2.58e-01,  2.31e-01,  8.37e-02,  4.10e-01,  6.97e-01,\n",
       "           4.39e-01,  5.35e-01,  1.10e+00,  1.09e+00,  8.31e-01,  9.54e-01,\n",
       "           1.34e+00,  1.07e+00,  1.26e+00,  8.49e-01,  9.63e-01,  1.08e+00,\n",
       "           6.81e-01,  2.81e-01,  2.02e-02, -3.06e-01, -8.20e-01, -1.04e+00,\n",
       "          -9.05e-01, -1.00e+00, -2.90e-01, -4.44e-01, -4.27e-01,  1.01e-02,\n",
       "          -1.73e-01, -7.51e-01, -6.22e-01, -6.46e-01, -1.03e+00, -1.03e+00,\n",
       "          -9.01e-01, -1.44e+00, -9.74e-01, -1.30e+00, -1.07e+00, -8.50e-01,\n",
       "          -8.21e-01, -5.62e-01, -3.60e-01, -2.44e-01,  3.85e-01,  2.31e-01,\n",
       "           7.76e-01,  6.82e-01,  6.00e-01,  1.18e+00,  7.83e-01,  1.10e+00])}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional: \n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "# inspect one sample in the dataset\n",
    "print(dataset_mdfp[9984]) # ['x'].shape\n",
    "\n",
    "# check reloaded dataset\n",
    "with open(f'{DATASET_SAVE_FOLDER}/unscaled/mdfp_dataset_{name}.p', 'rb') as f:\n",
    "    unpkled_dataset = pickle.load(f)\n",
    "unpkled_dataset[9984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28950e0c-4f32-4249-a81a-95fe71c4d547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

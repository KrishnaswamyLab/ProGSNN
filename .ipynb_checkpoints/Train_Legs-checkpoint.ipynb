{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNet and Scatter (with Optional Edge Feature Support)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "\n",
    "# TODO (alex) this is pretty inefficient using a for loop, is there a faster way to do this?\n",
    "# TODO (alex) only scatter higher diffusions using masking\n",
    "def scatter_moments(graph, batch_indices, moments_returned=4):\n",
    "    \"\"\" Compute specified statistical coefficients for each feature of each graph passed. The graphs expected are disjoint subgraphs within a single graph, whose feature tensor is passed as argument \"graph.\"\n",
    "        \"batch_indices\" connects each feature tensor to its home graph.\n",
    "        \"Moments_returned\" specifies the number of statistical measurements to compute. If 1, only the mean is returned. If 2, the mean and variance. If 3, the mean, variance, and skew. If 4, the mean, variance, skew, and kurtosis.\n",
    "        The output is a dictionary. You can obtain the mean by calling output[\"mean\"] or output[\"skew\"], etc.\"\"\"\n",
    "    # Step 1: Aggregate the features of each mini-batch graph into its own tensor\n",
    "    graph_features = [torch.zeros(0) for i in range(torch.max(batch_indices) + 1)]\n",
    "    for i, node_features in enumerate(\n",
    "        graph\n",
    "    ):  # Sort the graph features by graph, according to batch_indices. For each graph, create a tensor whose first row is the first element of each feature, etc.\n",
    "        #        print(\"node features are\",node_features)\n",
    "        if (\n",
    "            len(graph_features[batch_indices[i]]) == 0\n",
    "        ):  # If this is the first feature added to this graph, fill it in with the features.\n",
    "            graph_features[batch_indices[i]] = node_features.view(\n",
    "                -1, 1, 1\n",
    "            )  # .view(-1,1,1) changes [1,2,3] to [[1],[2],[3]],so that we can add each column to the respective row.\n",
    "        else:\n",
    "            graph_features[batch_indices[i]] = torch.cat(\n",
    "                (graph_features[batch_indices[i]], node_features.view(-1, 1, 1)), dim=1\n",
    "            )  # concatenates along columns\n",
    "\n",
    "    statistical_moments = {\"mean\": torch.zeros(0)}\n",
    "    if moments_returned >= 2:\n",
    "        statistical_moments[\"variance\"] = torch.zeros(0)\n",
    "    if moments_returned >= 3:\n",
    "        statistical_moments[\"skew\"] = torch.zeros(0)\n",
    "    if moments_returned >= 4:\n",
    "        statistical_moments[\"kurtosis\"] = torch.zeros(0)\n",
    "\n",
    "    for data in graph_features:\n",
    "        data = data.squeeze()\n",
    "        def m(i):  # ith moment, computed with derivation data\n",
    "            return torch.sum(deviation_data ** i, axis=1) / torch.sum(\n",
    "                torch.ones(data.shape), axis=1\n",
    "            )\n",
    "\n",
    "        mean = torch.sum(data, axis=1) / torch.sum(torch.ones(data.shape), axis=1)\n",
    "        if moments_returned >= 1:\n",
    "            statistical_moments[\"mean\"] = torch.cat(\n",
    "                (statistical_moments[\"mean\"], mean[None, ...]), dim=0\n",
    "            )\n",
    "\n",
    "        # produce matrix whose every row is data row - mean of data row\n",
    "        tuple_collect = []\n",
    "        for a in mean:\n",
    "            mean_row = torch.ones(data.shape[1]) * a\n",
    "            tuple_collect.append(\n",
    "                mean_row[None, ...]\n",
    "            )  # added dimension to concatenate with differentiation of rows\n",
    "        # each row contains the deviation of the elements from the mean of the row\n",
    "        deviation_data = data - torch.cat(tuple_collect, axis=0)\n",
    "\n",
    "        # variance: difference of u and u mean, squared element wise, summed and divided by n-1\n",
    "        variance = m(2)\n",
    "        if moments_returned >= 2:\n",
    "            statistical_moments[\"variance\"] = torch.cat(\n",
    "                (statistical_moments[\"variance\"], variance[None, ...]), dim=0\n",
    "            )\n",
    "\n",
    "        # skew: 3rd moment divided by cubed standard deviation (sd = sqrt variance), with correction for division by zero (inf -> 0)\n",
    "        skew = m(3) / (variance ** (3 / 2))\n",
    "        skew[\n",
    "            skew > 1000000000000000\n",
    "        ] = 0  # multivalued tensor division by zero produces inf\n",
    "        skew[\n",
    "            skew != skew\n",
    "        ] = 0  # single valued division by 0 produces nan. In both cases we replace with 0.\n",
    "        if moments_returned >= 3:\n",
    "            statistical_moments[\"skew\"] = torch.cat(\n",
    "                (statistical_moments[\"skew\"], skew[None, ...]), dim=0\n",
    "            )\n",
    "\n",
    "        # kurtosis: fourth moment, divided by variance squared. Using Fischer's definition to subtract 3 (default in scipy)\n",
    "        kurtosis = m(4) / (variance ** 2) - 3\n",
    "        kurtosis[kurtosis > 1000000000000000] = -3\n",
    "        kurtosis[kurtosis != kurtosis] = -3\n",
    "        if moments_returned >= 4:\n",
    "            statistical_moments[\"kurtosis\"] = torch.cat(\n",
    "                (statistical_moments[\"kurtosis\"], kurtosis[None, ...]), dim=0\n",
    "            )\n",
    "\n",
    "\n",
    "    # Concatenate into one tensor (alex)\n",
    "    statistical_moments = torch.cat([v for k,v in statistical_moments.items()], axis=1)\n",
    "    return statistical_moments\n",
    "\n",
    "\n",
    "class LazyLayer(torch.nn.Module):\n",
    "    \"\"\" Currently a single elementwise multiplication with one laziness parameter per\n",
    "    channel. this is run through a softmax so that this is a real laziness parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.Tensor(2, n))\n",
    "\n",
    "    def forward(self, x, propogated):\n",
    "        inp = torch.stack((x, propogated), dim=1)\n",
    "        s_weights = torch.nn.functional.softmax(self.weights, dim=0)\n",
    "        return torch.sum(inp * s_weights, dim=-2)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.ones_(self.weights)\n",
    "        \n",
    "def gcn_norm(edge_index, edge_weight=None, num_nodes=None,\n",
    "             add_self_loops=False, dtype=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                 device=edge_index.device)\n",
    "\n",
    "    if add_self_loops:\n",
    "        edge_index, tmp_edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, 1, num_nodes)\n",
    "        assert tmp_edge_weight is not None\n",
    "        edge_weight = tmp_edge_weight\n",
    "\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n",
    "    deg_inv_sqrt = deg.pow_(-1)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "    return edge_index, deg_inv_sqrt[row] * edge_weight\n",
    "\n",
    "\n",
    "class Diffuse(MessagePassing):\n",
    "    \"\"\" Implements low pass walk with optional weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, trainable_laziness=False, fixed_weights=True\n",
    "    ):\n",
    "        super().__init__(aggr=\"add\", node_dim=-3)  # \"Add\" aggregation.\n",
    "        assert in_channels == out_channels\n",
    "        self.trainable_laziness = trainable_laziness\n",
    "        self.fixed_weights = fixed_weights\n",
    "        if trainable_laziness:\n",
    "            self.lazy_layer = LazyLayer(in_channels)\n",
    "        if not self.fixed_weights:\n",
    "            self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        # turn off this step for simplicity\n",
    "        if not self.fixed_weights:\n",
    "            x = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization\n",
    "        edge_index, edge_weight = gcn_norm(edge_index, edge_weight, x.size(self.node_dim), dtype=x.dtype)\n",
    "\n",
    "        # Step 4-6: Start propagating messages.\n",
    "        propogated = self.propagate(\n",
    "            edge_index, edge_weight=edge_weight, size=None, x=x,\n",
    "        )\n",
    "        if not self.trainable_laziness:\n",
    "            return 0.5 * (x + propogated)\n",
    "        return self.lazy_layer(x, propogated)\n",
    "\n",
    "    def message(self, x_j, edge_weight):\n",
    "        # x_j has shape [E, out_channels]\n",
    "        # Step 4: Normalize node features.\n",
    "        return edge_weight.view(-1, 1, 1) * x_j\n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        # Step 6: Return new node embeddings.\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "def feng_filters():\n",
    "    tmp = np.arange(16).reshape(4,4) #tmp doesn't seem to be used!\n",
    "    results = [4]\n",
    "    for i in range(2, 4):\n",
    "        for j in range(0, i):\n",
    "            results.append(4*i+j)\n",
    "    return results\n",
    "\n",
    "\n",
    "class Scatter(torch.nn.Module):\n",
    "    def __init__(self, in_channels, edge_in_channels = None, trainable_laziness=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.edge_in_channels = edge_in_channels\n",
    "        self.trainable_laziness = trainable_laziness\n",
    "        self.diffusion_layer1 = Diffuse(in_channels, in_channels, trainable_laziness=trainable_laziness)\n",
    "        self.diffusion_layer2 = Diffuse(\n",
    "            4 * in_channels, 4 * in_channels, trainable_laziness=trainable_laziness\n",
    "        ) # No edge channels the second time.\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #print(f\"SHall we use edge features? The user specified {self.edge_in_channels}. Does the data have edge attributes?{hasattr(data,'edge_attr')}\")\n",
    "        if hasattr(data,'edge_attr') and self.edge_in_channels: # if you've specified edge channels and the data has them, we'll use them!\n",
    "          #print(\"We're using edge attributes!\")\n",
    "          edge_attr = data.edge_attr\n",
    "        else:\n",
    "          edge_attr = None;\n",
    "    \n",
    "        avgs = [x[:,:,None]]\n",
    "        for i in range(16):\n",
    "            avgs.append(self.diffusion_layer1(avgs[-1], edge_index)) # when diffusing over the graph, we use edge features\n",
    "        filter1 = avgs[1] - avgs[2]\n",
    "        filter2 = avgs[2] - avgs[4]\n",
    "        filter3 = avgs[4] - avgs[8]\n",
    "        filter4 = avgs[8] - avgs[16]\n",
    "        s0 = avgs[0]\n",
    "        s1 = torch.abs(torch.cat([filter1, filter2, filter3, filter4], dim=-1))\n",
    "\n",
    "        avgs = [s1]\n",
    "        for i in range(16):\n",
    "            avgs.append(self.diffusion_layer2(avgs[-1], edge_index)) # When diffusing over our filters, we don't/\n",
    "        filter1 = avgs[1] - avgs[2]\n",
    "        filter2 = avgs[2] - avgs[4]\n",
    "        filter3 = avgs[4] - avgs[8]\n",
    "        filter4 = avgs[8] - avgs[16]\n",
    "        s2 = torch.abs(torch.cat([filter1, filter2, filter3, filter4], dim=1))\n",
    "        s2_reshaped = torch.reshape(s2, (-1, self.in_channels, 4))\n",
    "        s2_swapped = torch.reshape(torch.transpose(s2_reshaped, 1, 2), (-1, 16, self.in_channels))\n",
    "        s2 = s2_swapped[:, feng_filters()]\n",
    "\n",
    "        x = torch.cat([s0, s1], dim=2)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = torch.cat([x, s2], dim=1)\n",
    "\n",
    "        # x = scatter_mean(x, batch, dim=0)\n",
    "        if hasattr(data, 'batch'):\n",
    "            x = scatter_moments(x, data.batch, 4)\n",
    "        else:\n",
    "            x = scatter_moments(x, torch.zeros(data.x.shape[0], dtype=torch.int32), 4)\n",
    "        return x\n",
    "\n",
    "    def out_shape(self):\n",
    "        # x * 4 moments * in\n",
    "        return 11 * 4 * self.in_channels\n",
    "\n",
    "\n",
    "class TSNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, edge_in_channels = None, trainable_laziness=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.edge_in_channels = edge_in_channels\n",
    "        self.trainable_laziness = trainable_laziness\n",
    "        self.scatter = Scatter(in_channels, edge_in_channels = edge_in_channels, trainable_laziness=trainable_laziness)\n",
    "        self.lin1 = Linear(self.scatter.out_shape(), out_channels)\n",
    "        self.act = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.scatter(data)\n",
    "        x = self.act(x)\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Transform <__main__.ClusteringCoefficient object at 0x7f7e9979f460>\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric.data.storage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2133989/3359896601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./results/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2133989/3359896601.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(run_args, out_file)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;31m#         return dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEShaw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graphs/total_graphs.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"splits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/protein_trajectory_project/de_shaw_Dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, transform)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         self.amino_acid_dict = {'MET' : 0,\\\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric.data.storage'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.utils\n",
    "from torch.nn import Linear\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.transforms import Compose\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from tqdm import trange\n",
    "from de_shaw_Dataset import DEShaw\n",
    "import os\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "class NetworkXTransform(object):\n",
    "    def __init__(self, cat=False):\n",
    "        self.cat = cat\n",
    "\n",
    "    def __call__(self, data):\n",
    "        x = data.x\n",
    "        netx_data = to_networkx(data)\n",
    "        ecc = self.nx_transform(netx_data)\n",
    "        nx.set_node_attributes(netx_data, ecc, 'x')\n",
    "        ret_data = from_networkx(netx_data)\n",
    "        ret_x = ret_data.x.view(-1, 1).type(torch.float32)\n",
    "        if x is not None and self.cat:\n",
    "            x = x.view(-1, 1) if x.dim() == 1 else x\n",
    "            data.x = torch.cat([x, ret_x], dim=-1)\n",
    "        else:\n",
    "            data.x = ret_x\n",
    "        return data\n",
    "\n",
    "    def nx_transform(self, networkx_data):\n",
    "        \"\"\" returns a node dictionary with a single attribute\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Eccentricity(NetworkXTransform):\n",
    "    def nx_transform(self, data):\n",
    "        return nx.eccentricity(data)\n",
    "\n",
    "\n",
    "class ClusteringCoefficient(NetworkXTransform):\n",
    "    def nx_transform(self, data):\n",
    "        return nx.clustering(data)\n",
    "\n",
    "\n",
    "def get_transform(name):\n",
    "    if name == \"eccentricity\":\n",
    "        transform = Eccentricity()\n",
    "    elif name == \"clustering_coefficient\":\n",
    "        transform = ClusteringCoefficient()\n",
    "    elif name == \"scatter\":\n",
    "        transform = Compose([Eccentricity(), ClusteringCoefficient(cat=True)])\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown transform %s\" % name)\n",
    "    return transform\n",
    "\n",
    "\n",
    "def split_dataset(dataset, splits=(0.8, 0.1, 0.1), seed=0):\n",
    "    \"\"\" Splits data into non-overlapping datasets of given proportions.\n",
    "    \"\"\"\n",
    "    splits = np.array(splits)\n",
    "    splits = splits / np.sum(splits)\n",
    "    n = len(dataset)\n",
    "    torch.manual_seed(seed)\n",
    "    val_size = int(splits[1] * n)\n",
    "    test_size = int(splits[2] * n)\n",
    "    train_size = n - val_size - test_size\n",
    "    #ds = dataset.shuffle()\n",
    "    ds = dataset\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "    #return ds[:train_size], ds[train_size : train_size + val_size], ds[-test_size:]\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "\n",
    "def accuracy(model, dataset,loss_fn, name):\n",
    "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data)\n",
    "        total_loss += loss_fn(pred,data.y)\n",
    "    # correct = float(pred.eq(data.y).sum().item())\n",
    "    acc = total_loss / len(dataset)\n",
    "    return acc, pred\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    \"\"\" Early Stopping pytorch implementation from Stefano Nardo https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d \"\"\"\n",
    "    def __init__(self, mode='min', min_delta=0, patience=8, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if metrics != metrics: # slight modification from source, to handle non-tensor metrics. If NAN, return True.\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)\n",
    "\n",
    "def evaluate(model,loss_fn, train_ds, test_ds, val_ds):\n",
    "    train_acc, train_pred = accuracy(model, train_ds,loss_fn, \"Train\")\n",
    "    test_acc, test_pred = accuracy(model, test_ds,loss_fn, \"Test\")\n",
    "    val_acc, val_pred = accuracy(model, val_ds,loss_fn, \"Test\")\n",
    "    results = {\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_pred\": train_pred,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_pred\": test_pred,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_pred\": val_pred,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def train_model(run_args, out_file):\n",
    "\n",
    "    if \"transform\" in run_args:\n",
    "        transform = get_transform(run_args[\"transform\"])\n",
    "    else:\n",
    "        transform = None\n",
    "    print(f\"Using Transform {transform}\")\n",
    "\n",
    "    if run_args[\"dataset\"] in [\"COLLAB\", \"REDDIT-MULTI-5K\", \"IMDB-BINARY\",\"IMDB-MULTI\",\"BZR\",\"OHSU\",\"QM9\"]:\n",
    "#         dataset = TUDataset(\n",
    "#             root=\"\", name=run_args[\"dataset\"], pre_transform=transform, \n",
    "#             use_node_attr=True, use_edge_attr=True\n",
    "#         )\n",
    "        \n",
    "#         return dataset\n",
    "        dataset = DEShaw(\"graphs/total_graphs.pkl\")\n",
    "        train_ds, val_ds, test_ds = split_dataset(dataset,splits=args[\"splits\"])\n",
    "        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=8)\n",
    "\n",
    "    elif run_args[\"dataset\"] in [\"ogbg-molhiv\"]:\n",
    "        from ogb.graphproppred import PygGraphPropPredDataset\n",
    "        d_name = \"ogbg-molhiv\"\n",
    "        dataset = PygGraphPropPredDataset(name=run_args[\"dataset\"])\n",
    "\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=8)\n",
    "        valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False)\n",
    "\n",
    "    if run_args[\"model\"] == \"ts_net\":\n",
    "        model = TSNet(\n",
    "            dataset.num_node_features,\n",
    "            dataset.num_classes,\n",
    "            #edge_in_channels=dataset.num_edge_features,\n",
    "            trainable_laziness=False\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    early_stopper = EarlyStopping(mode = 'max',patience=5,percentage=True)\n",
    "\n",
    "    results_compiled = []\n",
    "    early_stopper = EarlyStopping(mode = 'min',patience=5,percentage=False)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in trange(1, 300 + 1):\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss = loss_fn(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            results = evaluate(model, loss_fn, train_ds, test_ds, val_ds)\n",
    "            print('Epoch:', epoch, results['train_acc'], results['test_acc'])\n",
    "            results_compiled.append(results['test_acc'])\n",
    "            #torch.save(results, '%s_%d.%s' % (out_file, epoch, out_end))\n",
    "            if early_stopper.step(results['val_acc']):\n",
    "                print(\"Early stopping criterion met. Ending training.\")\n",
    "                break # if the validation accuracy decreases for eight consecutive epochs, break. \n",
    "    model.eval()\n",
    "    results = evaluate(model, loss_fn, train_ds, test_ds, val_ds)\n",
    "    print(\"Results compiled:\",results_compiled)\n",
    "    print('saving scatter model')\n",
    "    torch.save(model.scatter.state_dict(), str(out_file) + \"learnable_scat_model(redo).npy\")\n",
    "    torch.save(results, str(out_file) + \"results(redo).pth\")\n",
    "\n",
    "args = {\n",
    "    \"dataset\": \"QM9\",\n",
    "    \"model\": \"ts_net\",\n",
    "    \"model_args\": {\n",
    "        \"epsilon\": 1e-16,\n",
    "        \"num_layers\": 1\n",
    "    },\n",
    "    \"model_dir\": \"/home/atong/trainable_scattering/models/v1/0\",\n",
    "    \"transform\": \"clustering_coefficient\", # QM9 contains \"infinite path lengths,\" which the eccentricity\n",
    "    \"splits\":(0.8,0.1,0.1)\n",
    "}\n",
    "\n",
    "\n",
    "dataset = train_model(args, './results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[33].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch_geometric.data\n",
    "\n",
    "from LEGS_module import Scatter\n",
    "\n",
    "#from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "import networkx as nx\n",
    "from pysmiles import read_smiles\n",
    "\n",
    "class ZINCTranch(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, file_name, transform=None):\n",
    "        tranch = np.load(file_name, allow_pickle=True)\n",
    "        \n",
    "        self.tranch = tranch.item()\n",
    "        self.smi = list(tranch.item().keys())\n",
    "        self.props = list(tranch.item().values())\n",
    "        self.transform = transform\n",
    "        self.num_node_features = 1\n",
    "        self.num_classes = 4\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.tranch)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "             \n",
    "        \n",
    "        smi = self.smi[idx]\n",
    "            \n",
    "        prop_dict = self.tranch[smi]\n",
    "        \n",
    "        props = np.zeros(4)\n",
    "\n",
    "        props[0] = prop_dict['MinEStateIndex']\n",
    "        props[1] = prop_dict['MolWt']\n",
    "        props[2] = prop_dict['MinPartialCharge']\n",
    "        props[3] = prop_dict['TPSA']\n",
    "        \n",
    "        mol = read_smiles(smi)\n",
    "\n",
    "        data = from_networkx_custom(mol)\n",
    "        data.y = torch.tensor([props]).float()\n",
    "        \n",
    "        feats = []\n",
    "        for i in range(data.num_nodes):\n",
    "            feats.append([1.])\n",
    "        data.x = torch.tensor(feats).float()\n",
    "        data.edge_attr = None\n",
    "\n",
    "        if self.transform: \n",
    "            return self.transform(data)\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "class Scattering(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        model = Scatter(1, edge_in_channels = None, trainable_laziness=None)\n",
    "        model.load_state_dict(torch.load(\"/home/jacksongrady/graphGeneration/gsae/gsae/LEGS/resultslearnable_scat_model.npy\"))\n",
    "        model.eval()\n",
    "        self.model = model\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        props = sample.y\n",
    "        scat = self.model(sample)\n",
    "        \n",
    "        return scat[0], props[0]\n",
    "        \n",
    "        \n",
    "def from_networkx_custom(G):\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    G = G.to_directed() if not nx.is_directed(G) else G\n",
    "    edge_index = torch.LongTensor(list(G.edges)).t().contiguous()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for i, (_, feat_dict) in enumerate(G.nodes(data=True)):\n",
    "        for key, value in feat_dict.items():\n",
    "            if(str(key) != \"stereo\"):\n",
    "                data[str(key)] = [value] if i == 0 else data[str(key)] + [value]\n",
    "\n",
    "    for i, (_, _, feat_dict) in enumerate(G.edges(data=True)):\n",
    "        for key, value in feat_dict.items():\n",
    "            data[str(key)] = [value] if i == 0 else data[str(key)] + [value]\n",
    "\n",
    "    for key, item in data.items():\n",
    "        try:\n",
    "            data[key] = torch.tensor(item)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    data['edge_index'] = edge_index.view(2, -1)\n",
    "    data = torch_geometric.data.Data.from_dict(data)\n",
    "    data.num_nodes = G.number_of_nodes()\n",
    "\n",
    "    return data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ZINCTranch(\"../data/dict_tranches/HBCD.npy\", transform=Scattering())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████████████████████████████████████████████████████████▊                                                                    | 1276/2469 [00:09<00:08, 144.28it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "scat_mom_list = []\n",
    "\n",
    "for index, entry in enumerate(tqdm(dataset)):\n",
    "        scat_mom_list.append(entry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranch = np.load(\"./data/dict_tranches/2000_sample_BBAB.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
